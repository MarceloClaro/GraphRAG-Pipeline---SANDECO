
# GraphRAG Pipeline Visualizer: Arquitetura de Recupera√ß√£o Aumentada por Grafos

![Status](https://img.shields.io/badge/Status-Auditoria_T√©cnica-blue?style=for-the-badge)
![Tech Stack](https://img.shields.io/badge/Stack-React_|_Gemini_AI_|_D3.js_|_TensorFlow-indigo?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

## üìë 1. Resumo Executivo

Este reposit√≥rio hospeda a implementa√ß√£o de refer√™ncia de uma pipeline **GraphRAG (Graph-based Retrieval-Augmented Generation)**. Diferentemente de sistemas RAG tradicionais, que dependem exclusivamente de busca vetorial (*vector search*) baseada em similaridade de cosseno em um espa√ßo latente plano, esta arquitetura constr√≥i um **Grafo de Conhecimento Estruturado** a partir de documentos n√£o estruturados (PDFs).

O sistema integra **Modelos de Linguagem Grande (LLMs - Google Gemini 2.0)** para extra√ß√£o sem√¢ntica, **Redes Neurais Convolucionais (CNNs)** com fun√ß√£o de perda **Triplet Loss** para refinamento de embeddings, e algoritmos de **Teoria dos Grafos** para detec√ß√£o de comunidades e centralidade. O objetivo prim√°rio √© mitigar alucina√ß√µes estoc√°sticas e permitir infer√™ncias do tipo *multi-hop* (conex√£o l√≥gica de conceitos distantes) atrav√©s de topologia expl√≠cita.

---

## üõ†Ô∏è 2. Arquitetura da Pipeline (Metodologia)

A pipeline √© segmentada em 4 est√°gios macro, subdivididos em processos at√¥micos audit√°veis. Abaixo detalha-se o funcionamento t√©cnico, a justificativa te√≥rica e o diferencial de engenharia de cada etapa.

### 2.1. Ingest√£o e Pr√©-processamento Sem√¢ntico (Stage: UPLOAD)

#### Objetivo
Transforma√ß√£o de arquivos PDF bin√°rios em unidades de texto process√°veis (*chunks*), preservando rigorosamente a hierarquia documental e o contexto sem√¢ntico.

#### Procedimento T√©cnico
1.  **Extra√ß√£o via PDF.js:** Leitura bruta dos bytes e convers√£o para string, com tratamento de *encoding*.
2.  **Limpeza Heur√≠stica:** Remo√ß√£o de artefatos de OCR, hifens de quebra de linha e cabe√ßalhos/rodap√©s repetitivos que introduzem ru√≠do no espa√ßo vetorial.
3.  **Chunking Hier√°rquico:** Segmenta√ß√£o baseada na estrutura l√≥gica do documento (ex: Artigos Jur√≠dicos, Se√ß√µes Acad√™micas), em detrimento da contagem arbitr√°ria de tokens.
4.  **Enriquecimento via LLM (Gemini 2.0 Flash):** Cada chunk √© submetido a uma infer√™ncia para:
    *   **Classifica√ß√£o Taxon√¥mica:** (ex: "Defini√ß√£o", "Metodologia", "Inciso Legal").
    *   **Reconhecimento de Entidades Nomeadas (NER):** Extra√ß√£o de palavras-chave fundamentais.
    *   **Rotulagem Sint√©tica:** Gera√ß√£o de t√≠tulos descritivos para facilitar a indexa√ß√£o.

#### üí° Diferencial & Justificativa
O *Naive Chunking* (corte fixo a cada $N$ tokens) fragmenta contextos sem√¢nticos, prejudicando a recupera√ß√£o. Nossa abordagem hier√°rquica preserva a unidade de sentido (o "√°tomo" de informa√ß√£o). O enriquecimento via LLM injeta metadados expl√≠citos que n√£o existem no texto bruto, aumentando a precis√£o da vetoriza√ß√£o subsequente.

---

### 2.2. Vetoriza√ß√£o e Embeddings (Stage: EMBEDDINGS)

#### Objetivo
Mapeamento do texto enriquecido para vetores num√©ricos de alta dimens√£o (*High-Dimensional Vectors*), convertendo linguagem natural em representa√ß√µes matem√°ticas process√°veis.

#### Procedimento T√©cnico
*   **Modelo Base:** `text-embedding-004` (Google Gemini) ou fallback para `Sentence-BERT`.
*   **Input Rico (Rich Input):** O vetor n√£o √© gerado apenas do corpo do texto. A entrada √© concatenada da seguinte forma:
    $$Input = [Tipo_{Entidade}] \oplus [Keywords] \oplus [Conte√∫do]$$
*   **Dimensionalidade:** 768 dimens√µes.

#### üí° Diferencial & Justificativa
Ao incorporar metadados (tipo e keywords) no input do embedding, for√ßa-se o modelo vetorial a "atentar" para as entidades principais e a estrutura, n√£o apenas para a sintaxe da frase. Isso resulta em vetores que agrupam melhor por t√≥pico e fun√ß√£o.

---

### 2.3. Refinamento Vetorial via CNN e Triplet Loss (Otimiza√ß√£o)

#### Objetivo
Ajuste fino (*Fine-Tuning*) das posi√ß√µes dos vetores no hiperespa√ßo para maximizar a coes√£o intraclasse e a separa√ß√£o interclasse, utilizando Aprendizado Supervisionado por M√©tricas.

#### Procedimento T√©cnico
1.  **Arquitetura:** Implementa√ß√£o de uma **CNN 1D** otimizada para sequ√™ncias.
2.  **Fun√ß√£o de Perda (Loss Function):** Utiliza√ß√£o da **Triplet Loss**.
    $$L(A, P, N) = \max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha, 0)$$
    *   Onde $A$ √© a √¢ncora, $P$ √© positivo (mesma classe/keyword) e $N$ √© negativo (classe distinta). $\alpha$ √© a margem de separa√ß√£o.
3.  **Valida√ß√£o Cruzada (Cross-Validation):**
    *   **Estrat√©gia de Split 80/20:** 80% dos vetores comp√µem o conjunto de treino (onde ocorre a retropropaga√ß√£o do gradiente) e 20% formam o conjunto de valida√ß√£o (para monitoramento de generaliza√ß√£o).
    *   **Otimizador:** AdamW com decaimento de peso (*weight decay*) para regulariza√ß√£o.

#### üí° Diferencial & Justificativa
Embeddings pr√©-treinados (como o da OpenAI ou Google) s√£o gen√©ricos. Nosso refinamento adapta a distribui√ß√£o espacial dos vetores ao **dom√≠nio espec√≠fico** dos documentos carregados. O uso de Triplet Loss √© o estado da arte (SOTA) para aprendizado de representa√ß√µes, garantindo que conceitos semanticamente similares fiquem matematicamente pr√≥ximos.

---

### 2.4. Clusteriza√ß√£o e Constru√ß√£o do Grafo (Stage: CLUSTERING & GRAPH)

#### Objetivo
Transforma√ß√£o da nuvem de pontos vetorial em uma estrutura topol√≥gica de n√≥s e arestas, permitindo an√°lise de rede.

#### Procedimento T√©cnico (Clusteriza√ß√£o)
*   **Algoritmo:** K-Means++ com determina√ß√£o din√¢mica de $K$ ($\approx \sqrt{N/2}$).
*   **Valida√ß√£o:** C√°lculo do **Silhouette Score** para medir a consist√™ncia dos agrupamentos.
*   **Proje√ß√£o:** Redu√ß√£o de dimensionalidade para visualiza√ß√£o 2D (similar a UMAP).

#### Procedimento T√©cnico (Arestas H√≠bridas)
A conex√£o entre dois n√≥s ($A$ e $B$) n√£o √© bin√°ria. O peso da aresta $W_{AB}$ √© calculado por uma fun√ß√£o de custo composta:

$$W_{AB} = (\text{Overlap}(A,B) \times 0.6) + (\text{Jaccard}(A,B) \times 0.4)$$

*   **Interse√ß√£o Sem√¢ntica (Jaccard):** Baseada nas palavras-chave extra√≠das pela IA.
*   **Coeficiente de Sobreposi√ß√£o (Overlap):** √ötil para detectar rela√ß√µes de subconjunto (hierarquia).
*   **Filtro de Confian√ßa:** Arestas com $W_{AB} < 0.35$ s√£o descartadas para reduzir ru√≠do (sparsification).

#### üí° Diferencial & Justificativa
A maioria dos RAGs utiliza apenas *K-Nearest Neighbors (KNN)*. N√≥s criamos arestas expl√≠citas baseadas em **vocabul√°rio compartilhado** e **topologia**. Isso permite detectar comunidades tem√°ticas (ex: cluster de "Direito Penal") e calcular m√©tricas de centralidade (identificando os conceitos "Hub" do documento).

---

## üìä 3. M√©tricas de Auditoria e Qualidade

O sistema gera automaticamente um **Relat√≥rio T√©cnico (Qualis A1)** contendo indicadores fundamentais para valida√ß√£o cient√≠fica:

1.  **Modularidade (Q):** Mede a for√ßa da divis√£o do grafo em m√≥dulos. $Q > 0.4$ indica estrutura comunit√°ria robusta.
2.  **Densidade do Grafo:** Raz√£o entre arestas existentes e poss√≠veis. Controla a dispers√£o da informa√ß√£o.
3.  **Silhouette Score:** Valida√ß√£o da consist√™ncia dos clusters (intervalo -1 a 1). Valores > 0.5 indicam alta coes√£o.
4.  **Centralidade (Degree/Betweenness):** Identifica√ß√£o matem√°tica dos n√≥s mais influentes na rede.

---

## üöÄ 4. Guia de Reprodutibilidade

Para reproduzir os experimentos apresentados, siga os passos abaixo:

### Pr√©-requisitos
*   Node.js v18 ou superior.
*   Chave de API v√°lida do Google Gemini (`GEMINI_API_KEY`).

### Instala√ß√£o e Configura√ß√£o

```bash
# 1. Clonar o reposit√≥rio
git clone https://github.com/seu-user/graphrag-visualizer.git

# 2. Instalar depend√™ncias
npm install

# 3. Configurar Vari√°veis de Ambiente
# Crie um arquivo .env na raiz do projeto
echo "API_KEY=sua_chave_aqui" > .env

# 4. Inicializar a Aplica√ß√£o
npm start
```

### Protocolo de Execu√ß√£o do Experimento
1.  Acesse a interface em `http://localhost:3000`.
2.  **Etapa 1:** Realize o upload de PDFs (Artigos Cient√≠ficos ou Leis recomendados).
3.  **Etapa 1 (A√ß√£o):** Clique em "Limpar & Classificar com Gemini" para ativar a pipeline de NLP.
4.  **Etapa 2:** Gere os embeddings. Em seguida, configure os hiperpar√¢metros da CNN (sugerido: *Learning Rate 0.005, Margin 0.2, Mining Strategy: Hard*) e inicie o **Treinamento**. Monitore as curvas de perda de Treino vs. Valida√ß√£o.
5.  **Etapa 3:** Execute a Clusteriza√ß√£o e analise a distribui√ß√£o espacial.
6.  **Etapa 4:** Construa o Grafo. Utilize o painel de "An√°lise de Clusters" para inspecionar os t√≥picos emergentes.
7.  **Exporta√ß√£o:** Baixe o Relat√≥rio T√©cnico e as visualiza√ß√µes em PNG de alta resolu√ß√£o.

---

## ‚ö†Ô∏è 5. Limita√ß√µes Conhecidas

*   **Custo Computacional Client-Side:** O refinamento da CNN √© executado no navegador. Para datasets massivos (>10k chunks), recomenda-se a migra√ß√£o para um backend Python (PyTorch/TensorFlow).
*   **Depend√™ncia de LLM:** A qualidade final do grafo √© diretamente proporcional √† qualidade da extra√ß√£o de entidades realizada pelo Gemini na Etapa 1.
*   **Janela de Contexto:** Refer√™ncias que cruzam chunks muito distantes podem perder a conex√£o direta se n√£o houver vocabul√°rio compartilhado expl√≠cito.

---

## üë®‚Äçüíª 6. Autoria e Cr√©ditos

Desenvolvido como prova de conceito para arquiteturas avan√ßadas de Sistemas de Recupera√ß√£o de Informa√ß√£o.

*   **Engenharia de Prompt:** Otimizada para Gemini 2.0 Flash.
*   **Visualiza√ß√£o de Dados:** D3.js Force Simulation e Recharts.
*  **AUTOR :** Prof. Marcelo Claro Laranjeira
*  **Padr√£o de Projeto:** Programa√ß√£o Reativa Funcional (React Hooks).
